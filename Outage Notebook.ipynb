{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Outages Energy Consumption Statistical Analysis\n",
    "\n",
    "**Name(s)**: Diego Osborn\n",
    "\n",
    "**Website Link**: https://d2osborn.github.io/Power-Outages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pio.renderers.default = 'notebook'\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "from dsc80_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction and Question Identification\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the data you have access to. Brainstorm a few questions that interest you about the dataset. Pick one question you plan to investigate further. (As the data science lifecycle tells us, this question may change as you work on your project.)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What are the characteristics of major power outages with higher severity? Variables to consider include location, time, climate, land-use characteristics, electricity consumption patterns, economic characteristics, etc. What risk factors may an energy company want to look into when predicting the location and severity of its next major power outage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data appropriately. For instance, you may need to replace data that should be missing with NaN or create new columns out of given ones (e.g. compute distances, scale data, or get time information from time stamps).\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outage = pd.read_excel('outage.xlsx')\n",
    "outage = outage.iloc[4:, 1:]\n",
    "outage = outage.reset_index(drop=True)\n",
    "outage.columns = outage.iloc[0]\n",
    "outage = outage.iloc[2:, :]\n",
    "outage = outage.reset_index(drop=True)\n",
    "\n",
    "def change_to_time(date, time):\n",
    "    combined = pd.to_datetime(date[date.notna()].astype(str) + ' ' + time[time.notna()].astype(str), \n",
    "                              errors='coerce')\n",
    "    return combined\n",
    "\n",
    "outage['OUTAGE.START'] = change_to_time(outage['OUTAGE.START.DATE'], outage['OUTAGE.START.TIME'])\n",
    "outage['OUTAGE.RESTORATION'] = change_to_time(outage['OUTAGE.RESTORATION.DATE'], \n",
    "                                              outage['OUTAGE.RESTORATION.TIME'])\n",
    "\n",
    "outage = outage.drop(columns=['OUTAGE.START.DATE','OUTAGE.START.TIME','OUTAGE.RESTORATION.DATE',\n",
    "                              'OUTAGE.RESTORATION.TIME'])\n",
    "\n",
    "datetime_cols = ['OUTAGE.START', 'OUTAGE.RESTORATION']\n",
    "\n",
    "str_cols = ['U.S._STATE', 'POSTAL.CODE', 'NERC.REGION', \n",
    "            'CLIMATE.REGION', 'CLIMATE.CATEGORY', 'CAUSE.CATEGORY', \n",
    "            'CAUSE.CATEGORY.DETAIL', 'HURRICANE.NAMES']\n",
    "outage[str_cols] = outage[str_cols].astype('string')\n",
    "\n",
    "int_cols = ['OBS', 'YEAR', 'OUTAGE.DURATION', \n",
    "            'DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.SALES', \n",
    "            'COM.SALES', 'IND.SALES', 'TOTAL.SALES', \n",
    "            'RES.CUSTOMERS', 'COM.CUSTOMERS', 'IND.CUSTOMERS', \n",
    "            'TOTAL.CUSTOMERS', 'PC.REALGSP.STATE', 'PC.REALGSP.USA', \n",
    "            'UTIL.REALGSP', 'TOTAL.REALGSP', 'POPULATION']\n",
    "outage[int_cols] = outage[int_cols].astype('Int64')\n",
    "\n",
    "float_cols = [col for col in outage.columns if col not in str_cols and col not in int_cols \n",
    "              and col not in datetime_cols]\n",
    "outage[float_cols] = outage[float_cols].astype('Float64')\n",
    "\n",
    "outage[['OUTAGE.DURATION', \n",
    "        'DEMAND.LOSS.MW', \n",
    "        'CUSTOMERS.AFFECTED']] = outage[['OUTAGE.DURATION', 'DEMAND.LOSS.MW', \n",
    "                                         'CUSTOMERS.AFFECTED']].replace(0, np.nan)\n",
    "\n",
    "outage = outage.applymap(lambda x: np.nan if pd.isnull(x) else x)\n",
    "\n",
    "sunrise = datetime.time(6, 0, 0)\n",
    "sunset = datetime.time(20, 0, 0)\n",
    "outage['IS_DARK'] = np.where(outage['OUTAGE.START'].isnull(), np.nan, \n",
    "                             (outage['OUTAGE.START'].dt.time >= sunset) | \n",
    "                             (outage['OUTAGE.START'].dt.time <= sunrise))\n",
    "\n",
    "outage = outage.drop(columns=['CAUSE.CATEGORY.DETAIL', 'HURRICANE.NAMES'])\n",
    "\n",
    "outage = outage[(outage['U.S._STATE'] != 'Hawaii') & (outage['U.S._STATE'] != 'Alaska')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distributions of relevant columns separately by using DataFrame operations and drawing at least two relevant plots.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of Outages during Day and Night Times\n",
    "dark_distribution = outage['IS_DARK'].value_counts().reset_index()\n",
    "dark_distribution.columns = ['IS_DARK', 'COUNT']\n",
    "fig = px.bar(dark_distribution, x='IS_DARK', y='COUNT', color='IS_DARK', \n",
    "             labels={'IS_DARK': 'Night Time', 'COUNT': 'Count'})\n",
    "fig.update_layout(title='Distribution of Outages during Day and Night Times', \n",
    "                  xaxis=dict(tickmode='array', tickvals=[False, True], ticktext=['Day', 'Night']))\n",
    "\n",
    "# Distribution of Total Sales\n",
    "fig = px.histogram(outage, x='TOTAL.SALES', title='Distribution of Total Sales')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the statistics of pairs of columns to identify possible associations. For instance, you may create scatter plots and plot conditional distributions, or box-plots. You must plot at least two such plots in your notebook. The results of your bivariate analyses will be helpful in identifying interesting hypothesis tests!\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count of Outage Events per Month\n",
    "outage_count_by_month = outage.groupby('MONTH')['OUTAGE.DURATION'].count()\n",
    "fig = px.bar(x=outage_count_by_month.index, y=outage_count_by_month.values,\n",
    "             labels={'x': 'Month', 'y': 'Count of Outages'},\n",
    "             title='Count of Outage Events per Month')\n",
    "fig.update_xaxes(type='category')\n",
    "\n",
    "# Median Outage Duration per Month\n",
    "outage_count_by_month = outage.groupby('MONTH')['OUTAGE.DURATION'].median()\n",
    "fig = px.bar(x=outage_count_by_month.index, y=outage_count_by_month.values,\n",
    "             labels={'x': 'Month', 'y': 'Outage Duration (MEDIAN)'},\n",
    "             title='Median Outage Duration per Month')\n",
    "fig.update_xaxes(type='category')\n",
    "\n",
    "# Median Outage Duration per State\n",
    "outage_fig = outage.groupby('POSTAL.CODE')['OUTAGE.DURATION'].median()\n",
    "outage_fig = pd.DataFrame(outage_fig).reset_index()\n",
    "fig = px.choropleth(\n",
    "    outage_fig, \n",
    "    locations=\"POSTAL.CODE\",\n",
    "    locationmode=\"USA-states\", \n",
    "    color=\"OUTAGE.DURATION\", \n",
    "    scope=\"usa\",\n",
    "    color_continuous_scale='reds',\n",
    "    labels={'TOTAL.SALES': 'Median Outage Duration per State'}\n",
    ")\n",
    "\n",
    "# Total Sales vs Customers Affected\n",
    "outage_sampled = outage.sample(1000, random_state=42)\n",
    "fig = px.scatter(outage_sampled, x='TOTAL.SALES', y='CUSTOMERS.AFFECTED', \n",
    "                  title='Total Sales vs Customers Affected',\n",
    "                  labels={'TOTAL.SALES': 'Total Sales', 'CUSTOMERS.AFFECTED': 'Customers Affected'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose columns to group and pivot by and examine aggregate statistics.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Severity per NERC Region Group Table\n",
    "outage_agg = outage.groupby('NERC.REGION')[['OUTAGE.DURATION', \n",
    "                                            'DEMAND.LOSS.MW', \n",
    "                                            'CUSTOMERS.AFFECTED']].median()\n",
    "\n",
    "# Average Outage Duration between CLIMATE.REGION and CLIMATE.CATEGORY\n",
    "outage_avg_pivot = outage.pivot_table(index='CLIMATE.REGION', columns = 'CLIMATE.CATEGORY', \n",
    "                                      values = 'OUTAGE.DURATION', aggfunc = 'mean')\n",
    "\n",
    "# Joint Distribution Outage Duration between CLIMATE.REGION and CLIMATE.CATEGORY\n",
    "outage_joint = outage.pivot_table(index='CLIMATE.REGION', columns = 'CLIMATE.CATEGORY', \n",
    "                                  values = 'OUTAGE.DURATION', aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a column in the dataset with non-trivial missingness to analyze, and perform permutation tests to analyze the dependency of the missingness of this column on other columns.\n",
    "\n",
    "Specifically, find at least one other column that the missingness of your selected column does depend on, and at least one other column that the missingness of your selected column does not depend on.\n",
    "\n",
    "Tip: Make sure you know the difference between the different types of missingness before approaching that section. Many students in the past have lost credit for mistaking one type of missingness for another.\n",
    "\n",
    "Note that some datasets may have special requirements for this section; look at the “Special Considerations” section of your chosen dataset for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month Dist\n",
    "outage_consumption = outage.copy()\n",
    "outage_consumption['TOTAL_SALES_MISSING'] = outage_consumption['TOTAL.SALES'].isna()\n",
    "\n",
    "month_dist = (\n",
    "    outage_consumption\n",
    "    .assign(total_sales_missing=outage['TOTAL.SALES'].isna())\n",
    "    .pivot_table(index='MONTH', columns='total_sales_missing', aggfunc='size')\n",
    ")\n",
    "month_dist.columns = ['total_sales_missing = False', 'total_sales_missing = True']\n",
    "month_dist = month_dist / month_dist.sum()\n",
    "\n",
    "n_repetitions = 1_000\n",
    "shuffled = outage_consumption.copy()\n",
    "\n",
    "tvds = []\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled['MONTH'] = np.random.permutation(shuffled['MONTH'])\n",
    "    pivoted = (\n",
    "        shuffled\n",
    "        .pivot_table(index='MONTH', columns='TOTAL_SALES_MISSING', aggfunc='size')\n",
    "    )\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds.append(tvd)\n",
    "\n",
    "observed_tvd = month_dist.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "fig = px.histogram(pd.DataFrame(tvds), x=0, nbins=25, histnorm='probability', \n",
    "                   title='Empirical Distribution of the TVD')\n",
    "fig.add_vline(x=observed_tvd, line_color='red', line_width=5, \n",
    "              annotation_text=f'Observed TVD = {round(observed_tvd, 2)}', \n",
    "              annotation_position='top right')\n",
    "fig.update_layout(margin=dict(t=60))\n",
    "\n",
    "p_value = (np.array(tvds) >= observed_tvd).mean()\n",
    "# fig.write_html('tvd-month-sales.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# NERC Dist\n",
    "outage_consumption = outage.copy()\n",
    "outage_consumption['TOTAL_SALES_MISSING'] = outage_consumption['TOTAL.SALES'].isna()\n",
    "\n",
    "nerc_category_dist = (\n",
    "    outage_consumption\n",
    "    .assign(total_sales_missing=outage['TOTAL.SALES'].isna())\n",
    "    .pivot_table(index='NERC.REGION', columns='total_sales_missing', aggfunc='size')\n",
    ")\n",
    "nerc_category_dist.columns = ['total_sales_missing = False', 'total_sales_missing = True']\n",
    "nerc_category_dist = nerc_category_dist / nerc_category_dist.sum()\n",
    "\n",
    "n_repetitions = 1_000\n",
    "shuffled = outage_consumption.copy()\n",
    "\n",
    "tvds = []\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled['NERC.REGION'] = np.random.permutation(shuffled['NERC.REGION'])\n",
    "    pivoted = (\n",
    "        shuffled\n",
    "        .pivot_table(index='NERC.REGION', columns='TOTAL_SALES_MISSING', aggfunc='size')\n",
    "    )\n",
    "    pivoted = pivoted / pivoted.sum()\n",
    "    tvd = pivoted.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "    tvds.append(tvd)\n",
    "\n",
    "observed_tvd = nerc_category_dist.diff(axis=1).iloc[:, -1].abs().sum() / 2\n",
    "\n",
    "fig = px.histogram(pd.DataFrame(tvds), x=0, nbins=25, histnorm='probability', \n",
    "                   title='Empirical Distribution of the TVD')\n",
    "fig.add_vline(x=observed_tvd, line_color='red', line_width=5, \n",
    "              annotation_text=f'Observed TVD = {round(observed_tvd, 2)}', \n",
    "              annotation_position='top right')\n",
    "fig.update_layout(margin=dict(t=60))\n",
    "\n",
    "p_value = (np.array(tvds) >= observed_tvd).mean()\n",
    "# fig.write_html('tvd-nerc-sales.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "source": [
    "Clearly state a pair of hypotheses and perform a hypothesis test or permutation test that is not related to missingness. Feel free to use one of the example questions stated in the “Example Questions and Prediction Problems” section of your dataset’s description page or pose a hypothesis test of your own.\t\n",
    "\n",
    "**Null Hypothesis (H0)**: There is no significant difference in the distribution of `DEMAND.LOSS.MW` across different levels of `TOTAL.SALES`.\n",
    "\n",
    "**Alternative Hypothesis (H1)**: Higher levels of `TOTAL.SALES` are associated with greater `DEMAND.LOSS.MW`.\n",
    "\n",
    "**Test Statistic**: Difference in group means between `DEMAND.LOSS.MW` of high and low levels of `TOTAL.SALES`.\n",
    "\n",
    "**Significance Level**: Standard 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypothesis_outage = outage.copy()\n",
    "\n",
    "median_sales = hypothesis_outage['TOTAL.SALES'].median()\n",
    "hypothesis_outage['SALES_CATEGORY'] = np.where(hypothesis_outage['TOTAL.SALES'] >= \n",
    "                                               median_sales, 'High Sales', 'Low Sales')\n",
    "\n",
    "mean_weights = hypothesis_outage.groupby('SALES_CATEGORY')['DEMAND.LOSS.MW'].mean()\n",
    "observed_difference = mean_weights['High Sales'] - mean_weights['Low Sales']\n",
    "\n",
    "n_repetitions = 10_000\n",
    "differences = []\n",
    "\n",
    "for _ in range(n_repetitions):\n",
    "    with_shuffled = hypothesis_outage.assign(Shuffled_Weights=\n",
    "                                             np.random.permutation(hypothesis_outage['DEMAND.LOSS.MW']))\n",
    "    group_means = with_shuffled.groupby('SALES_CATEGORY')['Shuffled_Weights'].mean()\n",
    "    difference = group_means.loc['High Sales'] - group_means.loc['Low Sales']\n",
    "    differences.append(difference)\n",
    "\n",
    "fig = px.histogram(\n",
    "    pd.DataFrame(differences, columns=['Difference']), x='Difference', histnorm='probability', \n",
    "    title='Empirical Distribution of the Mean Differences in Demand \\\n",
    "    Loss (MW) <br> (High Total Sales - Low Total Sales)', nbins=25)\n",
    "fig.add_vline(x=observed_difference, line_color='red', line_width=5, \n",
    "              annotation_text=f'Observed Difference = {round(observed_difference, 2)}', \n",
    "              annotation_position='top right')\n",
    "fig.update_layout(margin=dict(t=60))\n",
    "\n",
    "p_value = (np.array(differences) >= observed_difference).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a prediction problem. Feel free to use one of the example prediction problems stated in the “Example Questions and Prediction Problems” section of your dataset’s description page or pose a hypothesis test of your own. The prediction problem you come up with doesn’t have to be related to the question you were answering in Steps 1-4, but ideally, your entire project has some sort of coherent theme.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "source": [
    "Predict the electricity consumption of an area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a “baseline model” for your prediction task that uses at least two features. (For this requirement, two features means selecting at least two columns from your original dataset that you should transform). You can leave numerical features as-is, but you’ll need to take care of categorical columns using an appropriate encoding. Implement all steps (feature transforms and model training) in a single sklearn Pipeline.\n",
    "\n",
    "Note: Both now and in Step 7: Final Model, make sure to evaluate your model’s ability to generalize to unseen data!\n",
    "\n",
    "There is no “required” performance metric that your baseline model needs to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAR columns\n",
    "# Categorical\n",
    "outage['MONTH'] = outage['MONTH'].fillna(7.0)\n",
    "outage['CLIMATE.REGION'] = outage['CLIMATE.REGION'].fillna(outage['CLIMATE.REGION'].mode())\n",
    "outage['CLIMATE.CATEGORY'] = outage['CLIMATE.REGION'].fillna(outage['CLIMATE.REGION'].mode())\n",
    "outage['OUTAGE.START'] = outage['OUTAGE.START'].ffill()\n",
    "outage['OUTAGE.RESTORATION'] = outage['OUTAGE.RESTORATION'].ffill()\n",
    "outage['IS_DARK'] = np.where(outage['OUTAGE.START'].isnull(), np.nan, (outage['OUTAGE.START'].dt.time >= sunset) | (outage['OUTAGE.START'].dt.time <= sunrise))\n",
    "\n",
    "# Numeric\n",
    "def prob_impute(s):\n",
    "    s = s.copy()\n",
    "    num_null = s.isna().sum()\n",
    "    fill_values = np.random.choice(s.dropna(), num_null)\n",
    "    s[s.isna()] = fill_values\n",
    "    return s\n",
    "\n",
    "numerical = ['RES.PRICE', 'COM.PRICE', 'IND.PRICE', 'TOTAL.PRICE', 'RES.SALES', 'COM.SALES', 'IND.SALES', \n",
    "             'TOTAL.SALES', 'RES.PERCEN', 'COM.PERCEN', 'IND.PERCEN', 'POPDEN_UC', 'POPDEN_RURAL']\n",
    "\n",
    "outage['OUTAGE.DURATION'] = (outage.groupby('CAUSE.CATEGORY')['OUTAGE.DURATION'].transform(prob_impute))\n",
    "for i in numerical:\n",
    "    outage[i] = (outage.groupby('CLIMATE.CATEGORY')[i].transform(prob_impute))\n",
    "\n",
    "# MCAR columns\n",
    "def quantitative_distribution(data):\n",
    "    values = []\n",
    "    bins = []\n",
    "    hist, bin_edges = np.histogram(data.dropna())\n",
    "    hist = hist / len(data.dropna())\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        bins.append(bin_edges[i:i + 2])\n",
    "    for i in range(len(data) - data.count()):\n",
    "        selected_bin = bins[np.random.choice(len(bins), p=hist)]\n",
    "        values.append(np.random.uniform(selected_bin[0], selected_bin[1]))\n",
    "    return np.array(values)\n",
    "\n",
    "def impute_quantitative_column(column_data):\n",
    "    missing_values = column_data.isnull()\n",
    "    distribution = quantitative_distribution(column_data)\n",
    "    column_data[missing_values] = distribution[:sum(missing_values)]\n",
    "    return column_data\n",
    "\n",
    "outage['ANOMALY.LEVEL'] = impute_quantitative_column(outage['ANOMALY.LEVEL'])\n",
    "outage['DEMAND.LOSS.MW'] = impute_quantitative_column(outage['DEMAND.LOSS.MW'])\n",
    "outage['CUSTOMERS.AFFECTED'] = impute_quantitative_column(outage['CUSTOMERS.AFFECTED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                                  [&#x27;POPULATION&#x27;,\n",
       "                                                   &#x27;PCT_WATER_INLAND&#x27;,\n",
       "                                                   &#x27;PC.REALGSP.STATE&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;CLIMATE.REGION&#x27;,\n",
       "                                                   &#x27;POSTAL.CODE&#x27;,\n",
       "                                                   &#x27;NERC.REGION&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                                  [&#x27;POPULATION&#x27;,\n",
       "                                                   &#x27;PCT_WATER_INLAND&#x27;,\n",
       "                                                   &#x27;PC.REALGSP.STATE&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;, OneHotEncoder(),\n",
       "                                                  [&#x27;CLIMATE.REGION&#x27;,\n",
       "                                                   &#x27;POSTAL.CODE&#x27;,\n",
       "                                                   &#x27;NERC.REGION&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                 [&#x27;POPULATION&#x27;, &#x27;PCT_WATER_INLAND&#x27;,\n",
       "                                  &#x27;PC.REALGSP.STATE&#x27;]),\n",
       "                                (&#x27;cat&#x27;, OneHotEncoder(),\n",
       "                                 [&#x27;CLIMATE.REGION&#x27;, &#x27;POSTAL.CODE&#x27;,\n",
       "                                  &#x27;NERC.REGION&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;POPULATION&#x27;, &#x27;PCT_WATER_INLAND&#x27;, &#x27;PC.REALGSP.STATE&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;CLIMATE.REGION&#x27;, &#x27;POSTAL.CODE&#x27;, &#x27;NERC.REGION&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  ['POPULATION',\n",
       "                                                   'PCT_WATER_INLAND',\n",
       "                                                   'PC.REALGSP.STATE']),\n",
       "                                                 ('cat', OneHotEncoder(),\n",
       "                                                  ['CLIMATE.REGION',\n",
       "                                                   'POSTAL.CODE',\n",
       "                                                   'NERC.REGION'])])),\n",
       "                ('regressor', LinearRegression())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['CLIMATE.REGION', 'PC.REALGSP.STATE', 'YEAR', 'MONTH', 'NERC.REGION', \n",
    "            'POPULATION', 'PCT_WATER_INLAND', 'POSTAL.CODE']\n",
    "target = 'TOTAL.SALES'\n",
    "X = outage[features]\n",
    "y = outage[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), ['POPULATION', 'PCT_WATER_INLAND', 'PC.REALGSP.STATE']),\n",
    "        ('cat', OneHotEncoder(), ['CLIMATE.REGION', 'POSTAL.CODE', 'NERC.REGION'])])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Train vs Test R²: 0.9516, 0.8610\n",
      "Baseline Train vs Test RMSE: 1889837.53, 3207278.17\n"
     ]
    }
   ],
   "source": [
    "baseline_y_train_pred = pipeline.predict(X_train)\n",
    "baseline_y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "baseline_train_rmse = np.sqrt(mean_squared_error(y_train, baseline_y_train_pred))\n",
    "baseline_test_rmse = np.sqrt(mean_squared_error(y_test, baseline_y_test_pred))\n",
    "\n",
    "baseline_train_r2 = r2_score(y_train, baseline_y_train_pred)\n",
    "baseline_test_r2 = r2_score(y_test, baseline_y_test_pred)\n",
    "\n",
    "print(f'Baseline Train vs Test R²: {baseline_train_r2:.4f}, {baseline_test_r2:.4f}')\n",
    "print(f'Baseline Train vs Test RMSE: {baseline_train_rmse:.2f}, {baseline_test_rmse:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the train vs test metrics that our baseline linear regression model is overfitting (the training accuracy for both $R^2$ and RMSE is greater than the test accuracy for both evalutation metrics). This will need to be taken care of in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a “final” model that improves upon the “baseline” model you created in Step 2. Do so by engineering at least two new features from the data, on top of any categorical encodings you performed in Baseline Model Step. (For instance, you may use a StandardScaler on a quantitative column and a QuantileTransformer transformer on a different column to get two new features.) Again, implement all steps in a single sklearn Pipeline. While deciding what features to use, you must perform a search for the best hyperparameters (e.g. tree depth) to use amongst a list(s) of options, either by using GridSearchCV or through some manual iterative method. In your notebook, state which hyperparameters you plan to tune and why before actually tuning them.\n",
    "\n",
    "Optional: You are encouraged to try many different modeling algorithms for your final model (i.e. LinearRegression, RandomForestClassifier, Lasso, SVC, etc.) If you do this, make sure to clearly indicate in your notebook which model is your actual final model as that will be used to grade the above requirements.\n",
    "\n",
    "Note 1: When training your model, make sure you use the same unseen and seen datasets from your baseline model. This way, the evaluation metric you get on your final model can be compared to your baseline’s on the basis of the model itself and not the dataset it was trained on. Based on which method you use for hyperparameter tuning, this may mean that you will need to use some of your training data as your validation data. If this is the case, make sure to train your final model on the whole dataset prior to evaluation.\n",
    "\n",
    "Note 2: You will not be graded on “how much” your model improved from Step 6: Baseline Model to Step 7: Final Model. What you will be graded on is on whether or not your model improved, as well as your thoughtfulness and effort in creating features, along with the other points above.\n",
    "\n",
    "Note 3: Don’t try to improve your model’s performance just by blindly transforming existing features into new ones. Think critically about what each transformation you’re doing actually does. For example, there’s no use in using a StandardScaler transformer if your goal is to reduce the RMSE of a linear model: as we learned in DSC 40A, and in Lecture 15, standardizing features in a regression model does not change the model’s predictions, only its coefficients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1863854.18 -17627258.44  -3375602.02  -2390184.19  -2749878.56\n",
      "  -3938444.54  -5792159.8   -1738576.08  -2076454.17  -5225499.39]\n",
      "r2 = train: 0.9965192486607278, val: 0.9058612475686199, test: 0.965983818255527\n",
      "rmse = train: 502461.00230711506, val: 2635082.0456623794, test: 1623764.093781233\n"
     ]
    }
   ],
   "source": [
    "outage['POP.DENSITY.URBAN'] = outage['POPULATION'] / outage['AREAPCT_URBAN']\n",
    "outage['POP.DENSITY.URBAN'] = np.log1p(outage['POP.DENSITY.URBAN'])\n",
    "outage['AVG.MONTHLY.PRICE'] = outage[['RES.PRICE', 'COM.PRICE', 'IND.PRICE']].mean(axis=1)\n",
    "outage['MONTH.SIN'] = np.sin(2 * np.pi * outage['MONTH'] / 12)\n",
    "outage['MONTH.COS'] = np.cos(2 * np.pi * outage['MONTH'] / 12)\n",
    "\n",
    "features = ['CLIMATE.REGION', 'PC.REALGSP.STATE', 'NERC.REGION', #'MONTH.SIN', 'MONTH.COS',\n",
    "            'PCT_WATER_INLAND', 'POSTAL.CODE', 'POP.DENSITY.URBAN', 'AVG.MONTHLY.PRICE']\n",
    "            \n",
    "X = outage[features]\n",
    "y = outage[target]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), ['PCT_WATER_INLAND', 'PC.REALGSP.STATE', 'POP.DENSITY.URBAN', 'AVG.MONTHLY.PRICE']),#, 'MONTH.SIN', 'MONTH.COS']), \n",
    "                                               ('cat', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.REGION', 'POSTAL.CODE', 'NERC.REGION'])], remainder='passthrough')\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "# params = {'model__n_estimators': [100, 200, 500], 'model__max_depth': [10, 20, 30], \n",
    "#           'model__min_samples_split': [2, 5, 10], 'model__min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, error_score='raise')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "best_model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=10, scoring='neg_root_mean_squared_error')\n",
    "print(cv_scores)\n",
    "\n",
    "train_r2 = r2_score(y_train, best_model.predict(X_train))\n",
    "val_r2 = r2_score(y_val, best_model.predict(X_val))\n",
    "test_r2 = r2_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, best_model.predict(X_train)))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, best_model.predict(X_val)))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, best_model.predict(X_test)))\n",
    "\n",
    "print(f'r2 = train: {train_r2}, val: {val_r2}, test: {test_r2}')\n",
    "print(f'rmse = train: {train_rmse}, val: {val_rmse}, test: {test_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a “fairness analysis” of your Final Model from the previous step. That is, try and answer the question “does my model perform worse for individuals in Group X than it does for individuals in Group Y?”, for an interesting choice of X and Y.\n",
    "\n",
    "As always, when comparing some quantitative attribute (in this case, something like precision or RMSE) across two groups, we use a permutation test. Let’s illustrate how this works with an example. Let’s suppose we have a sample voter dataset with columns 'Name', 'Age', and 'Voted', among others. We build a classifier that predicts whether someone voted (1) or didn’t (0).\n",
    "\n",
    "Here, we’ll say our two groups are\n",
    "- “young people”, people younger than 40\n",
    "- “old people”, people older than 40\n",
    "\n",
    "Note that in this example, we manually created these groups by binarizing the 'Age' column in our dataset, and that’s fine. (Remember, the Binarizer transformer with a threshold of 40 can do this for us.)\n",
    "\n",
    "For our evaluation metric, we’ll choose precision. (In Week 10’s lectures, we’ll look at other evaluation metrics and related parity measures for classifiers; choose the one that is most appropriate to your prediction task. If you built a regression model, you cannot use classification metrics like precision or recall; instead, you must use RMSE or \n",
    "R^2.)\n",
    "\n",
    "Now, we must perform a permutation test. Before doing so, we must clearly state a null and an alternative hypothesis.\n",
    "- Null Hypothesis: Our model is fair. Its precision for young people and old people are roughly the same, and any differences are due to random chance.\n",
    "- Alternative Hypothesis: Our model is unfair. Its precision for young people is lower than its precision for old people.\n",
    "\n",
    "From here, you should be able to implement the necessary permutation test. The only other guidance we will provide you with is that you should not be modifying your model to produce different results when computing test statistics; use only your final fitted model from Final Model Step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:49:57) \n[Clang 16.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b12909ae3d420039fe0d665486d7a3bae20c5f4e9ed25a1740938c9f4691a450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
